{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFIVfu5HPOlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0547d3ba-8b76-43ad-d893-f49983708615"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Project/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cw6rin8pfr2z",
        "outputId": "326bd374-f8f6-4a44-dcc4-38206c09a931"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install sentencepiece\n",
        "!pip install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: asgiref==3.2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.10)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs==20.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (20.3.0)\n",
            "Requirement already satisfied: blis==0.7.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.7.4)\n",
            "Requirement already satisfied: bokeh==2.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: cachetools==4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: catalogue==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2020.6.20)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (7.1.2)\n",
            "Requirement already satisfied: colorama==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.4.4)\n",
            "Requirement already satisfied: cymem==2.0.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (2.0.5)\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (0.6.0)\n",
            "Requirement already satisfied: Django==3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (3.1)\n",
            "Requirement already satisfied: django-allauth==0.40.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (0.40.0)\n",
            "Requirement already satisfied: django-rest-framework==0.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (0.1.0)\n",
            "Requirement already satisfied: djangorestframework==3.10.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.10.3)\n",
            "Requirement already satisfied: dnspython==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (2.0.0)\n",
            "Requirement already satisfied: fastapi==0.60.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.60.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.3.3)\n",
            "Requirement already satisfied: google-auth==1.20.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.20.0)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.4.1)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.30.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (1.30.0)\n",
            "Requirement already satisfied: h11==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (0.9.0)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (2.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (1.7.0)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (1.1.1)\n",
            "Requirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (2.11.2)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 33)) (1.0.1)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (1.1.2)\n",
            "Requirement already satisfied: LASER==0.0.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (0.0.5)\n",
            "Requirement already satisfied: Markdown==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (1.1.1)\n",
            "Requirement already satisfied: motor==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (2.1.0)\n",
            "Requirement already satisfied: murmurhash==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (1.0.5)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (3.5)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (1.18.5)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 42)) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 43)) (3.3.0)\n",
            "Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (20.4)\n",
            "Collecting pandas==1.0.5\n",
            "  Using cached https://files.pythonhosted.org/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 46)) (1.0.1)\n",
            "Requirement already satisfied: pathy==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (0.4.0)\n",
            "Requirement already satisfied: Pillow==7.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 48)) (7.2.0)\n",
            "Requirement already satisfied: pluggy==0.13.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 49)) (0.13.1)\n",
            "Requirement already satisfied: preshed==3.0.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 50)) (3.0.5)\n",
            "Requirement already satisfied: protobuf==3.12.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (3.12.4)\n",
            "Requirement already satisfied: psutil==5.7.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (5.7.2)\n",
            "Requirement already satisfied: py==1.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 53)) (1.10.0)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 54)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 55)) (0.2.8)\n",
            "Requirement already satisfied: pydantic==1.7.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 56)) (1.7.3)\n",
            "Requirement already satisfied: pymongo==3.10.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (3.10.1)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 58)) (2.4.7)\n",
            "Requirement already satisfied: pytest==6.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (6.2.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 60)) (2.8.1)\n",
            "Requirement already satisfied: python3-openid==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 61)) (3.1.0)\n",
            "Requirement already satisfied: pytz==2020.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 62)) (2020.1)\n",
            "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 63)) (5.3.1)\n",
            "Requirement already satisfied: regex==2020.11.13 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 64)) (2020.11.13)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 65)) (2.24.0)\n",
            "Requirement already satisfied: requests-oauthlib==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 66)) (1.2.0)\n",
            "Requirement already satisfied: rsa==4.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 67)) (4.6)\n",
            "Requirement already satisfied: scikit-learn==0.24.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 68)) (0.24.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 69)) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 70)) (0.1.95)\n",
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 71)) (1.2.2)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 72)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 73)) (0.0)\n",
            "Requirement already satisfied: smart-open==3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 74)) (3.0.0)\n",
            "Requirement already satisfied: spacy==3.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 75)) (3.0.3)\n",
            "Requirement already satisfied: spacy-legacy==3.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 76)) (3.0.1)\n",
            "Requirement already satisfied: sqlparse==0.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 77)) (0.3.1)\n",
            "Requirement already satisfied: srsly==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 78)) (2.4.0)\n",
            "Requirement already satisfied: starlette==0.13.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 79)) (0.13.6)\n",
            "Requirement already satisfied: tensorboard==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 80)) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 81)) (1.7.0)\n",
            "Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 82)) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 83)) (2.3.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 84)) (1.1.0)\n",
            "Requirement already satisfied: thinc==8.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 85)) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 86)) (2.1.0)\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 87)) (0.10.2)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 88)) (1.7.1+cu101)\n",
            "Requirement already satisfied: tornado==6.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 89)) (6.0.4)\n",
            "Requirement already satisfied: tqdm==4.57.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 90)) (4.57.0)\n",
            "Requirement already satisfied: typer==0.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 91)) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions==3.7.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 92)) (3.7.4.2)\n",
            "Requirement already satisfied: ujson==4.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 93)) (4.0.2)\n",
            "Requirement already satisfied: urllib3==1.25.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 94)) (1.25.10)\n",
            "Requirement already satisfied: uvicorn==0.11.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 95)) (0.11.6)\n",
            "Requirement already satisfied: wasabi==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 96)) (0.8.2)\n",
            "Requirement already satisfied: websockets==8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 97)) (8.1)\n",
            "Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 98)) (1.0.1)\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 99)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 100)) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 3)) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.20.0->-r requirements.txt (line 23)) (53.0.0)\n",
            "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.6->-r requirements.txt (line 95)) (0.15.2)\n",
            "Requirement already satisfied: httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.6->-r requirements.txt (line 95)) (0.1.1)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.0.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas\n",
            "  Found existing installation: pandas 1.2.2\n",
            "    Uninstalling pandas-1.2.2:\n",
            "      Successfully uninstalled pandas-1.2.2\n",
            "Successfully installed pandas-1.0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Collecting pandas\n",
            "  Using cached https://files.pythonhosted.org/packages/4c/33/87b15a5baeeb71bd677da3579f907e97476c5247c0e56a37079843af5424/pandas-1.2.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2020.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.0.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "Successfully installed pandas-1.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWMaPuJnFAF_"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "from DeBERTa.DeBERTa.deberta import pretrained_models, tokenizers\n",
        "from deberta import DeBERTaReconfig\n",
        "import re\n",
        "import string\n",
        "\n",
        "file_path = pathlib.Path.cwd()\n",
        "\n",
        "pretrained_model_base = {\n",
        "    'model_class': pretrained_models['base'],\n",
        "    'model_path': file_path.joinpath('model', 'base', 'pytorch_model.bin'),\n",
        "    'model_config_path': file_path.joinpath('model', 'base', 'config.json'),\n",
        "    'model_vocab_path': file_path.joinpath('model', 'base', 'bpe_encoder.bin'),\n",
        "    'model_vocab_type': 'gpt2'\n",
        "}\n",
        "\n",
        "class DeBERTaTxtClassifier(torch.nn.Module):\n",
        "    def __init__(self, model_path, model_config_path, freeze_deberta=True):\n",
        "        super().__init__()\n",
        "        self.deberta = DeBERTaReconfig(model_path, model_config_path)\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(D_in, H),\n",
        "            torch.nn.ReLU(),\n",
        "            #torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        if freeze_deberta:\n",
        "            for param in self.deberta.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        outputs = self.deberta(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask, output_all_encoded_layers=False)\n",
        "        nextTensor = torch.empty(len(input_ids), 768, dtype=torch.float)\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0]\n",
        "        \n",
        "        for i in range(len(last_hidden_state_cls)):\n",
        "            nextTensor[i] = last_hidden_state_cls[i][0]\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(nextTensor)\n",
        "\n",
        "        return logits\n",
        "\n",
        "#setting model here. Change pretrained_model between the base and v2xxlarge\n",
        "pretrained_model = pretrained_model_base\n",
        "max_seq_len = 512\n",
        "vocab_path = pretrained_model['model_vocab_path']\n",
        "vocab_type = pretrained_model['model_vocab_type']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Bv4zKHVoxd",
        "outputId": "f0c6d6e3-7a34-45db-974a-46e1934b3833"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import string\r\n",
        "\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdqAIY0qYlo1",
        "outputId": "c6673a67-0f84-4dfd-bf25-77b07c9285c6"
      },
      "source": [
        "#data preprocessing\r\n",
        "ps = PorterStemmer()\r\n",
        "lem = WordNetLemmatizer()\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "        text = re.sub('[^a-zA-Z0-9]',' ', text)\r\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\r\n",
        "        text = text.lower()\r\n",
        "        return text\r\n",
        "\r\n",
        "def text_process(text):\r\n",
        "    text = text.split()\r\n",
        "    text = [ps.stem(word) for word in text if not word in stopwords.words('english')]\r\n",
        "    text = ' '.join(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "def preprocess(filepath, basepath = './datasets/raw/'):\r\n",
        "    df = pd.read_csv(basepath + filepath)\r\n",
        "    df = df.dropna()\r\n",
        "    if (filepath == 'fake/train.csv'):\r\n",
        "        X = df.drop('label',axis=1)\r\n",
        "        y = df['label']\r\n",
        "        X = X.text\r\n",
        "        #X = fake_X.apply(clean_text).apply(text_process)\r\n",
        "    if (filepath == 'ag.csv'):\r\n",
        "        print(df)\r\n",
        "  \r\n",
        "    return X, y\r\n",
        "\r\n",
        "fake_X, fake_y = preprocess('fake/train.csv')\r\n",
        "print(fake_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        House Dem Aide: We Didn’t Even See Comey’s Let...\n",
            "1        Ever get the feeling your life circles the rou...\n",
            "2        Why the Truth Might Get You Fired October 29, ...\n",
            "3        Videos 15 Civilians Killed In Single US Airstr...\n",
            "4        Print \\nAn Iranian woman has been sentenced to...\n",
            "                               ...                        \n",
            "20795    Rapper T. I. unloaded on black celebrities who...\n",
            "20796    When the Green Bay Packers lost to the Washing...\n",
            "20797    The Macy’s of today grew from the union of sev...\n",
            "20798    NATO, Russia To Hold Parallel Exercises In Bal...\n",
            "20799      David Swanson is an author, activist, journa...\n",
            "Name: text, Length: 18285, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wMX01FcsNwW"
      },
      "source": [
        "def tokenize(input_sentences, max_seq_len = 512):\r\n",
        "    tokenized_sentences = {}\r\n",
        "    masks = {}\r\n",
        "    for key, sentence in input_sentences.items():\r\n",
        "        tokenizer = tokenizers[vocab_type](vocab_path)\r\n",
        "        tokens = tokenizer.tokenize(sentence)\r\n",
        "        tokens = tokens[:max_seq_len - 2]\r\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "        input_mask = [1]*len(input_ids)\r\n",
        "        paddings = max_seq_len-len(input_ids)\r\n",
        "        input_ids = input_ids + [0]*paddings\r\n",
        "        input_mask = input_mask + [0]*paddings\r\n",
        "        tokenized_sentences[key] = input_ids\r\n",
        "        masks[key] = input_mask\r\n",
        "    return tokenized_sentences, masks\r\n",
        "\r\n",
        "fake_tokenized, fake_masks = tokenize(fake_X)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cHTxkFpzeSa"
      },
      "source": [
        "import time\n",
        "import collections\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "\n",
        "def train(model, X_train, X_masks, Y_train):\n",
        "    X_train = collections.OrderedDict(sorted(X_train.items()))\n",
        "    xlist = []\n",
        "    ylist = []\n",
        "    masklist = []\n",
        "    for key, val in X_train.items():\n",
        "        xlist.append(val)\n",
        "        ylist.append(Y_train[key])\n",
        "        masklist.append(X_masks[key])\n",
        "    train_inputs = torch.tensor(xlist)\n",
        "    train_masks = torch.tensor(masklist)\n",
        "    train_labels = torch.tensor(ylist)\n",
        "    batch_size = 32\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    start = time.time()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                      lr=5e-5  \n",
        "                      )\n",
        "    epochs = 4\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    loss_fn = torch.nn.CrossEntropyLoss();\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t for t in batch)\n",
        "        model.zero_grad()\n",
        "        y_pred = model(b_input_ids, b_attn_mask)\n",
        "        loss = loss_fn(y_pred, b_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Finished training model in %.1f sec' % ((time.time()-start)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXP2dswPEouE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2398af0f-1585-43dc-a3f6-81c80c97e5dc"
      },
      "source": [
        "fake_model = DeBERTaTxtClassifier(pretrained_model['model_path'], pretrained_model['model_config_path'])\n",
        "train(fake_model, fake_tokenized, fake_masks, fake_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needs to be (state_dict post BERT config): 244\n",
            "is (model state): 203\n",
            "Finished training model in 6260.1 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}